{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a28638-c15b-40bf-be6f-916d84ae20ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gcloud config list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2927213b-95d3-4cc7-a57c-cf1bf650f240",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc9d4ade-0dcd-4814-ba6b-2f93e1dae055",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from vertexai import init\n",
    "from vertexai.preview.generative_models import GenerativeModel\n",
    "\n",
    "init(project=\"gen-lang-client-0947040378\", location=\"us-central1\")\n",
    "model = GenerativeModel(\"gemini-2.5-flash-preview-05-20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e29ff96-b583-411d-aaa0-016c925567c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import time\n",
    "from vertexai.preview.generative_models import GenerativeModel\n",
    "\n",
    "def extract_choices_from_prompt(text):\n",
    "    return set(re.findall(r\"\\b([ABC])\\s*:\", text.upper()))\n",
    "\n",
    "def extract_choices_from_response(text):\n",
    "    text = text.upper()\n",
    "    match = re.search(r\"RƒÇSPUNS(?:\\s+CORECT)?\\s*[:Ôºö]?\\s*([ABC](?:[\\s,]+[ABC])*)\", text)\n",
    "    if match:\n",
    "        return set(re.findall(r\"[ABC]\", match.group(1)))\n",
    "    return None\n",
    "\n",
    "def evaluate_gemini(model, dataset, max_examples=None, verbose=False, csv_path=None,multimodal=False):\n",
    "    results = []\n",
    "    exact_matches = 0\n",
    "    \n",
    "    for i, ex in enumerate(tqdm(dataset, desc=\"Evaluare QA\", unit=\"ex\")):\n",
    "        if max_examples and i >= max_examples:\n",
    "            break\n",
    "        try:\n",
    "            prompt = None\n",
    "            if multimodal:\n",
    "                image_text=str(ex.get(\"image\", \"\")).strip()\n",
    "                prompt = (\n",
    "                  image_text+\"\\n\"\n",
    "                  + ex[\"prompt\"].strip()\n",
    "                  + \"\\nMai √Ænt√¢i oferƒÉ doar litera/literele rƒÉspunsului corect sub forma:\\nRƒÉspuns: A\\n\"\n",
    "                  + \"Apoi oferƒÉ o explica»õie succintƒÉ pe un r√¢nd nou, fƒÉrƒÉ 'Let's think'.\\n\"\n",
    "                  + \"RƒÉspuns:\"\n",
    "                  )\n",
    "            else:\n",
    "                prompt = (\n",
    "                  ex[\"prompt\"].strip()\n",
    "                  + \"\\nMai √Ænt√¢i oferƒÉ doar litera/literele rƒÉspunsului corect sub forma:\\nRƒÉspuns: A\\n\"\n",
    "                  + \"Apoi oferƒÉ o explica»õie succintƒÉ pe un r√¢nd nou, fƒÉrƒÉ 'Let's think'.\\n\"\n",
    "                  + \"RƒÉspuns:\"\n",
    "                  )\n",
    "            gold = extract_choices_from_response(ex[\"response\"])\n",
    "\n",
    "            response = model.generate_content(\n",
    "                prompt\n",
    "            )\n",
    "\n",
    "            text = getattr(response, \"text\", \"\").strip()\n",
    "            pred = extract_choices_from_response(text)\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"\\nüìò Prompt: {prompt}\")\n",
    "                print(f\"‚úÖ Gold: {gold}\")\n",
    "                print(f\"üß† Pred: {pred}\")\n",
    "                print(f\"üìù Gemini raw: {text}\")\n",
    "\n",
    "            match = gold == pred if gold and pred else False\n",
    "            if match:\n",
    "                exact_matches += 1\n",
    "\n",
    "            results.append({\n",
    "                \"index\":str(i),\n",
    "                \"prompt\": ex[\"prompt\"].strip(),\n",
    "                \"golds\": \", \".join(sorted(gold)) if gold else \"\",\n",
    "                \"preds\": \", \".join(sorted(pred)) if pred else \"\",\n",
    "                \"gemini_raw\": text\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[‚ùå EROARE] {e}\")\n",
    "            continue\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = exact_matches / len(results) if results else 0\n",
    "    print(f\"\\nüìä Acurate»õe totalƒÉ (exact match): {accuracy:.2%}\")\n",
    "\n",
    "    # Export CSV\n",
    "    if csv_path:\n",
    "        df = pd.DataFrame(results)\n",
    "        df.to_csv(csv_path, index=False, encoding=\"utf-8\")\n",
    "        print(f\"üìÅ Rezultate salvate √Æn: {csv_path}\")\n",
    "\n",
    "    return results, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ed5004-cba5-4c15-b612-ace1d9705571",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "with open(\"multimodal.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = [json.loads(line) for line in f]\n",
    "    dataset_text_only = Dataset.from_list(dataset)\n",
    "\n",
    "    split_dataset = dataset_text_only.train_test_split(test_size=0.2, seed=42)\n",
    "    train_ds = split_dataset[\"train\"]\n",
    "    test_ds = split_dataset[\"test\"]\n",
    "    print(test_ds[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94276727-3893-407b-81b6-66b1ce451e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "results, acc = evaluate_gemini(\n",
    "    model,\n",
    "    dataset=test_ds,\n",
    "    max_examples=250,\n",
    "    verbose=True,\n",
    "    multimodal=True,\n",
    "    csv_path=\"gemini2.5_flash_qa_results.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bbcffc-91ac-4098-a470-7f3c483700eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected_indices = [38,51,63,111,199]\n",
    "filtered_test_ds = test_ds.select(selected_indices)\n",
    "results, acc = evaluate_gemini(\n",
    "    model,\n",
    "    dataset=filtered_test_ds,\n",
    "    max_examples=250,\n",
    "    verbose=True,\n",
    "    multimodal=True,\n",
    "    csv_path=\"completion_gemini2.5_flash_qa_results.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8eeb73b0-3431-4275-b9e2-50d9f31b74a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_judging_prompt(row):\n",
    "    return f\"\"\"\n",
    "E»ôti un evaluator expert √Æn QA. √é»õi dau un task QA:\n",
    "\n",
    "√éntrebare: {row['prompt']}\n",
    "RƒÉspuns corect: {row['golds']}\n",
    "RƒÉspuns prezis: {row['preds']}\n",
    "Explica»õie: {row['response']}\n",
    "\n",
    "Evalueaza NUMAI calitatea explica»õiei. Este explica»õia √Æn concordan»õƒÉ cu rƒÉspunsul prezis? Dar cu cel corect? Are legƒÉturƒÉ cu √Æntrebarea \n",
    "\n",
    "JudecƒÉ √Æn acest format:\n",
    "- Scor (√Æntre 0 »ôi 5): \n",
    "- Justificare:\n",
    "\"\"\"\n",
    "\n",
    "def extract_score(judgment_text):\n",
    "    match = re.search(r'Score\\s*\\(?\\d\\s*to\\s*5\\)?:?\\s*(\\d)', judgment_text)\n",
    "    return int(match.group(1)) if match else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039d6540-60ab-4490-a23b-7d096d82fb2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_files = glob.glob(\"*.csv\")\n",
    "\n",
    "for file in model_files:\n",
    "    df = pd.read_csv(file)\n",
    "    judgments = []\n",
    "    scores = []\n",
    "    if 'gemini_raw' in df.columns:\n",
    "        df.rename(columns={'gemini_raw': 'response'}, inplace=True)\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        try:\n",
    "            prompt = create_judging_prompt(row)\n",
    "            response = model.generate_content(prompt)\n",
    "            text = response.text.strip()\n",
    "            judgments.append(text)\n",
    "            scores.append(extract_score(text))\n",
    "            print(f'Completed row {i} for file {file}')\n",
    "        except Exception as e:\n",
    "            print(f\"Exception occured: {e}\")\n",
    "            \n",
    "    \n",
    "    df[\"judgment\"] = judgments\n",
    "    df[\"score\"] = scores\n",
    "    df.to_csv(file, index=False)\n",
    "    print(f\"Updated: {file}\")\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
